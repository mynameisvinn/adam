{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.utils import np_utils\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# model architecture \n",
    "n_dimensions = mnist.train.images[0].shape[0]\n",
    "n_classes = mnist.train.labels[0].shape[0]\n",
    "\n",
    "# adam hyperparams\n",
    "alpha = 0.01  # step size\n",
    "beta_1 = 0.9  # decay rate for 1st moment\n",
    "beta_2 = 0.999  # decay rate for 2nd moment... beta2 < beta1, which means m_t will be more sensitive to changes in gradient direction\n",
    "epsilon = 1e-8  # epsilon so we dont divide by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 32\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"inputs\"):\n",
    "    X_ = tf.placeholder(tf.float32, [None, n_dimensions], name=\"X\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "with tf.variable_scope(\"model\"):\n",
    "    fc1 = tf.layers.dense(inputs=X_, units=n_classes, name='fc1', activation=tf.nn.relu)  \n",
    "    \n",
    "with tf.variable_scope('model/fc1', reuse=True):\n",
    "    w1 = tf.get_variable('kernel')\n",
    "\n",
    "probs = tf.nn.softmax(fc1)\n",
    "loss = tf.losses.log_loss(labels=y_, predictions=probs)\n",
    "grad = tf.gradients(loss, w1)[0]  # raw gradients of loss function wrt params\n",
    "\n",
    "# initialize moments of gradients to zero\n",
    "m_t = tf.Variable(tf.zeros([784, 10]), name=\"first_moment\")\n",
    "v_t = tf.Variable(tf.zeros([784, 10]), name=\"second_moment\")\n",
    "\n",
    "with tf.variable_scope(\"first_moment\"):\n",
    "    # calculate moving average of gradient's 1st moment - this acts as a momentum term\n",
    "    # if the gradient points in the same direction, a momentum effect enlarges gradient\n",
    "    m_t_update = m_t.assign(beta_1 * m_t + (1 - beta_1) * grad)\n",
    "\n",
    "    # adjust for bias, which was introduced when m_t was initialized with zeros\n",
    "    m_t_adjusted = m_t_update / (1 - beta_1)\n",
    "\n",
    "with tf.variable_scope(\"second_moment\"):\n",
    "    # calculate moving average of gradient's 2nd moment\n",
    "    g2 = grad * grad  # element wise multiplication of gradient\n",
    "    v_t_update = v_t.assign(beta_2 * v_t + (1 - beta_2) * (g2))\n",
    "\n",
    "    # adjust for bias, which was introduced when v_t was initialized with zeros\n",
    "    v_t_adjusted = v_t_update / (1 - beta_2)\n",
    "\n",
    "# when gradients from successiveminibatches point in different direction, m_t will approach zero\n",
    "adam_update = m_t_adjusted/(tf.sqrt(v_t_adjusted) + epsilon)  # this is the paramater update vector\n",
    "adam_step = w1.assign(w1 - lr * adam_update)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_adam_loss_ = []\n",
    "ls_first_ = []  # track first moment\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # adam - calculate 1st and 2nd moments, \n",
    "        _, _ = sess.run([m_t_update, v_t_update], feed_dict={X_: batch_xs, y_: batch_ys})\n",
    "        \n",
    "        # then update parameter and calculate loss\n",
    "        _, loss_, m_t_ = sess.run([adam_step, loss, adam_update], feed_dict={X_: batch_xs, y_: batch_ys})\n",
    "        ls_adam_loss_.append(loss_)\n",
    "        ls_first_.append(m_t_)\n",
    "        \n",
    "# pretty plot\n",
    "rolling_plot = pd.Series(ls_adam_loss_).rolling(window=50).mean()\n",
    "plt.plot(rolling_plot)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"adam loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect magnitude of momentum vector... \n",
    "ls_magnitude = [np.linalg.norm(x) for x in ls_first_]\n",
    "\n",
    "rolling_plot = pd.Series(ls_magnitude).rolling(window=50).mean()\n",
    "plt.plot(rolling_plot)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"magnitude of first moment of gradient\")\n",
    "plt.title(\"magnitude of gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
